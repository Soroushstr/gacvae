Maccs.txt text file includes the 166-bit MACCS structures for the database

Run following python codes in a Colab environment
```Python
# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19nrPiFoCUzEzj0SWn2JtWHN9xwdOtQn3

# Installation of Required Packages
"""

! python -m pip install chemfp -i https://chemfp.com/packages/

os.chdir('/content/')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

# %tensorflow_version 1.15
import tensorflow as tf
print(tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

"""# MACCS FP array of molecules requried for cVAE training and testing"""

! cp /content/drive/MyDrive/maccs.txt /content/

"""# Defining Functions"""

def get_input_gi50_data(gi50_data_file_name, test_div=10, normalize=False, label = 16):

  data = np.array(pd.read_csv(gi50_data_file_name,header=None))
  data_maccs = np.array([list(x) for x in data[:,0]]).astype(np.int32)
  data[:,2] = np.round(data[:,2].astype(np.int32))
  data[:,2] = data[:,2]+abs(data[:,2].min())
  b = np.zeros((data[:,2].size, data[:,2].max()+1))
  b[np.arange(data[:,2].size),data[:,2].astype(np.int32)] = 1
  data = np.concatenate((data_maccs,b),axis=1)
  data = data.astype(np.float32)
  print('info: get_input_data: (gi50) data.shape -> {}'.format(data.shape))
  np.random.shuffle(data)
  test_size = data.shape[0] // test_div
  label_data = np.copy(data[:,-label:])
  #datadata = np.copy()
  if normalize:
    data[:,-1:] = (data[:,-1:] - data[:,-1:].min(axis=0)) / (data[:,-1:].max(axis=0) - data[:,-1:].min(axis=0))
  
  test_data, train_data = np.vsplit(data, [test_size])
  test_label, train_label = np.vsplit(label_data, [test_size])
  return(test_data,train_data,test_label,train_label)

data = pd.read_csv('maccs.txt', on_bad_lines='skip')
data = data[['NSC','NLOGGI50','FP']]
mols = data.copy()
data = np.concatenate((np.array([[int(x) for x in data.loc[y,'FP'].split(',')] for y in range(len(data))]),np.array(data[['NLOGGI50']])),axis=1)
data = data.astype(np.float32)
data_maccs = data[:,0]
data[:,-1] = np.round(data[:,-1].astype(np.int32))
data[:,-1] = data[:,-1]+abs(data[:,-1].min())
data = data.astype(np.int32)
data
#data = np.concatenate((data_maccs,b),axis=1)

data.shape

b = np.zeros((data[:,-1].size, data[:,-1].max()+1))
b[np.arange(data[:,-1].size),data[:,-1].astype(np.int32)] = 1
print(b.shape)
b

b[0,:]

def get_input_gi50_data(gi50_data_file_name, test_div=10, normalize=False, label = 18):

  data = pd.read_csv('maccs.txt', on_bad_lines='skip')
  data = data[['NSC','NLOGGI50','FP']]
  mols = data.copy()
  data = np.concatenate((np.array([[int(x) for x in data.loc[y,'FP'].split(',')] for y in range(len(data))]),np.array(data[['NLOGGI50']])),axis=1)
  data = data.astype(np.float32)
  data_maccs = data[:,0]
  data[:,-1] = np.round(data[:,-1].astype(np.int32))
  data[:,-1] = data[:,-1]+abs(data[:,-1].min())
  data = data.astype(np.int32)
  b = np.zeros((data[:,-1].size, data[:,-1].max()+1))
  b[np.arange(data[:,-1].size),data[:,-1].astype(np.int32)] = 1
  data = np.concatenate((data,b),axis=1)
  data = data.astype(np.float32)
  print('info: get_input_data: (gi50) data.shape -> {}'.format(data.shape))
  np.random.shuffle(data)
  test_size = data.shape[0] // test_div
  label_data = np.copy(data[:,-label:])
  feature = np.copy(data[:,0:-(label)])
  print(data.shape)
  data = np.delete(data,167,axis=1)
  data = np.delete(data,0,axis=1)
  print(data.shape)

  #datadata = np.copy()
  if normalize:
    data[:,-1:] = (data[:,-1:] - data[:,-1:].min(axis=0)) / (data[:,-1:].max(axis=0) - data[:,-1:].min(axis=0))
  
  test_data, train_data = np.vsplit(data, [test_size])
  test_label, train_label = np.vsplit(label_data, [test_size])
  return(test_data,train_data,test_label,train_label)

test_data, train_total_data, test_labels, train_label = get_input_gi50_data('maccs.txt')

train_label.shape

train_total_data.shape

import os
n_samples = train_total_data.shape[0]
print('info: test_labels.shape -> {}'.format(train_label.shape))
os.mkdir('/data/')
np.save('/data/test_data.npy',test_data)

os.mkdir('/content/results_cvae/')
RESULTS_DIR = '/content/results_cvae'

num_bits = 166
NUM_LABELS = 18
learning_rate = 0.001
dim_z = 20
n_hidden = 500
batch_size = 128
n_epochs = 300
NUM_TO_GENERATE = 20

with open(RESULTS_DIR + '/log.txt', 'a') as dec_fp:
        dec_fp.write('input x dimension : %d \n' % num_bits)
        dec_fp.write('input y label dimension : %d \n' % NUM_LABELS)
        dec_fp.write('2,4,6,8,10 <= -logGI50 < 3,5,7,9,11 \n')

        dec_fp.write('encoder layer [%d, %d] \n' % (n_hidden, n_hidden))
        dec_fp.write('encoder activation function [elu, tanh] \n')
        dec_fp.write('encoder dropout keep_prob [0.9, 0.9] \n')
        dec_fp.write('latent space z : %d \n' % dim_z)
        dec_fp.write('\n')
        dec_fp.write('decoder layer [%d, %d] \n' % (n_hidden, n_hidden))
        dec_fp.write('decoder activation function [tanh, elu] \n')
        dec_fp.write('decoder dropout keep_prob [0.9, 0.9] \n')
        dec_fp.write('output activation function [sigmoid] \n')
        dec_fp.write('\n')
        dec_fp.write('re-parameterization N(0,1) \n')
        dec_fp.write('batch size : %d \n' % batch_size)
        dec_fp.write('learning rate : %d \n' % learning_rate)

"""# **Network Architecture **"""

# input placeholders
x_hat = tf.placeholder(np.float32, shape=[None, num_bits], name='input_fp')
x = tf.placeholder(np.float32, shape=[None, num_bits], name='target_fp')
y = tf.placeholder(np.float32, shape=[None, NUM_LABELS], name='target_labels')

# dropout
keep_prob = tf.placeholder(np.float32, name='keep_prob')

# input for fp generation
z_in = tf.placeholder(np.float32, shape=[None, dim_z], name='latent_variable')
condition_in = tf.placeholder(np.float32, shape=[None, NUM_LABELS], name='conditional_vectors')  # condition

# Examples encoding
mu_ = tf.placeholder(np.float32, shape=[None, dim_z], name='latent_mu')
sigma_ = tf.placeholder(np.float32, shape=[None, dim_z], name='latent_sigma')
x_hat_ = tf.placeholder(np.float32, shape=[None, num_bits], name='ex_input_fp')
x_ = tf.placeholder(np.float32, shape=[None, num_bits], name='ex_target_fp')
y_ = tf.placeholder(np.float32, shape=[None, NUM_LABELS], name='ex_target_labels')



"""# **CVAE**"""

import tensorflow as tf


# Gaussian MLP as conditional encoder
def gaussian_MLP_conditional_encoder(x, y, n_hidden, n_output, keep_prob):
    with tf.variable_scope("gaussian_MLP_encoder", reuse=tf.AUTO_REUSE):
        # concatenate condition and image
        dim_y = int(y.get_shape()[1])
        input = tf.concat(axis=1, values=[x, y])

        # initializers
        w_init = tf.contrib.layers.variance_scaling_initializer()
        b_init = tf.constant_initializer(0.)

        # 1st hidden layer
        w0 = tf.get_variable('w0', [input.get_shape()[1], n_hidden+dim_y], initializer=w_init)
        b0 = tf.get_variable('b0', [n_hidden+dim_y], initializer=b_init)
        h0 = tf.matmul(input, w0) + b0
        h0 = tf.nn.elu(h0)
        h0 = tf.nn.dropout(h0, keep_prob)

        # 2nd hidden layer
        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)
        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)
        h1 = tf.matmul(h0, w1) + b1
        h1 = tf.nn.tanh(h1)
        h1 = tf.nn.dropout(h1, keep_prob)

        # output layer
        # borrowed from https: // github.com / altosaar / vae / blob / master / vae.py
        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output * 2], initializer=w_init)
        bo = tf.get_variable('bo', [n_output * 2], initializer=b_init)
        gaussian_params = tf.matmul(h1, wo) + bo

        # The mean parameter is unconstrained
        mean = gaussian_params[:, :n_output]
        # The standard deviation must be positive. Parametrize with a softplus and
        # add a small epsilon for numerical stability
        stddev = 1e-6 + tf.nn.softplus(gaussian_params[:, n_output:])

    return mean, stddev
  
# Bernoulli MLP as conditional decoder
def bernoulli_MLP_conditional_decoder(z, y, n_hidden, n_output, keep_prob, reuse=False):

    with tf.variable_scope("bernoulli_MLP_decoder", reuse=reuse):
        # concatenate condition and latent vectors
        input = tf.concat(axis=1, values=[z, y])

        # initializers
        w_init = tf.contrib.layers.variance_scaling_initializer()
        b_init = tf.constant_initializer(0.)

        # 1st hidden layer
        w0 = tf.get_variable('w0', [input.get_shape()[1], n_hidden], initializer=w_init)
        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)
        h0 = tf.matmul(input, w0) + b0
        h0 = tf.nn.tanh(h0)
        h0 = tf.nn.dropout(h0, keep_prob)

        # 2nd hidden layer
        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)
        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)
        h1 = tf.matmul(h0, w1) + b1
        h1 = tf.nn.elu(h1)
        h1 = tf.nn.dropout(h1, keep_prob)

        # output layer-mean
        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output], initializer=w_init)
        bo = tf.get_variable('bo', [n_output], initializer=b_init)
        y = tf.sigmoid(tf.matmul(h1, wo) + bo)

    return y


# Gateway
def autoencoder(x_hat, x, y, dim_img, dim_z, n_hidden, keep_prob):

    # encoding
    mu, sigma = gaussian_MLP_conditional_encoder(x_hat, y, n_hidden, dim_z, keep_prob)

    # sampling by re-parameterization technique
    z = mu + sigma * tf.random_normal(tf.shape(mu), 0, 1, dtype=tf.float32)

    # decoding
    x_ = bernoulli_MLP_conditional_decoder(z, y, n_hidden, dim_img, keep_prob)
    x_ = tf.clip_by_value(x_, 1e-6, 1 - 1e-6)

    # ELBO
    marginal_likelihood = tf.reduce_sum(x * tf.log(x_) + (1 - x) * tf.log(1 - x_), 1)
    KL_divergence = 0.5 * tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-6 + tf.square(sigma)) - 1, 1)

    marginal_likelihood = tf.reduce_mean(marginal_likelihood)
    KL_divergence = tf.reduce_mean(KL_divergence)

    ELBO = marginal_likelihood - KL_divergence

    # minimize loss instead of maximizing ELBO
    loss = -ELBO
    return mu, sigma, x_, z, loss, -marginal_likelihood, KL_divergence


# Conditional Decoder (Generator)
def decoder(z, y, dim_img, n_hidden):
    x_ = bernoulli_MLP_conditional_decoder(z, y, n_hidden, dim_img, 1.0, reuse=True)
    return x_

"""# **CVAE**

# New Section
"""

# network architecture
mu_, sigma_, x_, z, loss, neg_marginal_likelihood, KL_divergence = autoencoder(x_hat, x, y, num_bits, dim_z, n_hidden, keep_prob)
# optimization
train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)

# Encoding examples
mu__2, sigma__2 = gaussian_MLP_conditional_encoder(x_hat_, y_, n_hidden, dim_z, keep_prob)

""" training """

decoded = decoder(z_in, condition_in, num_bits, n_hidden)

encoded_mu, encoded_sigma = gaussian_MLP_conditional_encoder(x_hat_, y_, n_hidden, dim_z, keep_prob)

# train
total_batch = int(n_samples / batch_size)
print("n_samples : ", str(n_samples))
min_tot_loss = 1e99

latent_mus = []
latent_sigmas = []
```
with tf.Session() as sess:
   

        sess.run(tf.global_variables_initializer(), feed_dict={keep_prob: 0.9})

        for epoch in range(n_epochs):

            # Random shuffling
            np.random.shuffle(train_total_data)
            train_data_ = train_total_data[:, :-NUM_LABELS]
            # 0-1 to 0.001 - 0.999
            train_data_[train_data_[:] == 0] = 0.001
            train_data_[train_data_[:] == 1] = 0.999

            train_labels_ = train_total_data[:, -NUM_LABELS:]

            # Loop over all batches
            print("total_batch : ", str(total_batch))
            for i in range(total_batch):
                offset = (i * batch_size) % (n_samples)
                batch_xs_input = np.array(train_data_[offset:(offset + batch_size), :]).astype(np.float32)
                batch_ys_input = np.array(train_labels_[offset:(offset + batch_size)]).astype(np.float32)
                batch_xs_target = (batch_xs_input).astype(np.float32)

                # add salt & pepper noise
                if True:
                    batch_xs_input = batch_xs_input * np.random.randint(2, size=batch_xs_input.shape)
                    batch_xs_input += np.random.randint(2, size=batch_xs_input.shape)

                _, tot_loss, loss_likelihood, loss_divergence = sess.run(
                    (train_op, loss, neg_marginal_likelihood, KL_divergence),
                    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, y: batch_ys_input, keep_prob: np.array(0.9).astype(np.float32)})
                
            # print cost every epoch
            print("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f" % (
                epoch, tot_loss, loss_likelihood, loss_divergence))
            with open(RESULTS_DIR + '/log.txt', 'a') as epoch_log:
                epoch_log.write("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f \n" % (
                    epoch, tot_loss, loss_likelihood, loss_divergence))

            if epoch%50 == 0:
                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                latent_mus.append(mu_test)
                latent_sigmas.append(sigma_test)

            # if minimum loss is updated or final epoch, make results
            if min_tot_loss > tot_loss or epoch + 1 == n_epochs:
                min_tot_loss = tot_loss

                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                # Plot for analogical reasoning result
                if epoch > 3:
                    # onehot_label= [0:-3<<-2, 1:-2<<-1, 2:-1<<0, 3:0<<1, 4:1<<2, 5:2<<3, 6:3<<4, 7:4<<5, 8:5<<6, 9:6<<7, 10:7<<8, 11:8<<9, 12:9<<10, 13:10<<11, 14:11<<12, 15:12<<13, 16: 13<<14]

                    z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.
                    conditions_to_generate = np.zeros(shape=[NUM_TO_GENERATE, NUM_LABELS])
                    for i in range(NUM_TO_GENERATE):
                        label = i % NUM_LABELS
                        conditions_to_generate[i, label] = 1.0
                    x_output = sess.run(decoded, feed_dict={z_in: z,
                                                            condition_in: conditions_to_generate,
                                                            keep_prob: 1})
                    print('info: x_vector.shape -> {}'.format(x_output.shape))

                    x_output_bit = np.where(x_output > 0.5, 1, 0)

                    """ generate fingerprint """
                    # onehot_label = args.target_label+5
                    condition_gi = [4, 5, 6, 7, 8]
                    for i in condition_gi:
                        dec_fp = open(RESULTS_DIR + '/dec_fp_v' + str(i) + '_e%d.txt' % epoch, 'a')
                        for j, fp in enumerate(x_output_bit.tolist()):
                            fp_str = ""
                            if j % 18 == i + 3 + 1:
                                for bit in fp:
                                    fp_str += str(bit)
                                dec_fp.write(fp_str + ',%f' % i)
                                dec_fp.write('\n')
                        dec_fp.close()

قmu__1 = mu__.copy()
mu__2 = mu__.copy()
np.concatenate((mu__1,mu__2),axis=0).shape

"""# **Latent Space Projection**"""

z_test = mu_test + sigma_test * np.random.normal(0,1)

z_test

import matplotlib.pyplot as plt
embeddings = z_test.copy()
plt.figure(figsize=(8, 8))
plt.scatter(embeddings[:, 0] , embeddings[:, 1], alpha=0.5, s=2)
plt.xlabel("Dimension-1", size=20)
plt.ylabel("Dimension-2", size=20)
plt.xticks(size=20)
plt.yticks(size=20)
plt.title("Projection of 2D Latent-Space (Fashion-MNIST)", size=20)
plt.show()

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, init='pca', random_state=0)
X_tsne = tsne.fit_transform(embeddings)

plt.figure(figsize=(8, 8))
plt.scatter(X_tsne[:, 0] , X_tsne[:, 1], alpha=0.5, s=2)
plt.show()

X_tsne.shape





! unzip TGI.zip

original = pd.read_csv('maccs.txt', on_bad_lines='skip')
original

original['NLOGGI50'].min()

data = pd.read_csv('maccs.txt', on_bad_lines='skip')
data = data[['NSC','NLOGGI50','FP']]
mols = data.copy()
data = np.concatenate((np.array([[int(x) for x in data.loc[y,'FP'].split(',')] for y in range(len(data))]),np.array(data[['NLOGGI50']])),axis=1)
data = data.astype(np.float32)
data_maccs = data[:,0]
data[:,-1] = np.round(data[:,-1].astype(np.int32))
data[:,-1] = data[:,-1]+abs(data[:,-1].min())
data = data.astype(np.int32)

data = np.delete(data,0,1)
data.shape

test_gen = batch_xs_input_test.astype(np.int32)

t= ['0,'+','.join([str(x) for x in list(test_gen[c_,:])]) for c_ in range(len(test_gen))]
len(t)

gen_idx = [list(original['FP']).index('0,'+','.join([str(x) for x in list(test_gen[c_,:])])) for c_ in range(len(test_gen))]

giclasses = [np.round(x) for x in list(original.loc[gen_idx,'NLOGGI50'])]

import matplotlib.pyplot as plt
embeddings = z_test.copy()
plt.figure(figsize=(8, 8))
plt.scatter(embeddings[:, 0] , embeddings[:, 1], c=giclasses, alpha=0.5, s=2, cmap='tab20b')
plt.xlabel("Dimension-1", size=20)
plt.ylabel("Dimension-2", size=20)
plt.xticks(size=20)
plt.yticks(size=20)
plt.title("Projection of Latent-Space (NSCLC - NCI60)", size=20)
plt.colorbar()
plt.show()

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, init='pca', random_state=0)
X_tsne = tsne.fit_transform(embeddings)

plt.figure(figsize=(8, 8))
plt.scatter(X_tsne[:, 0] , X_tsne[:, 1], c=giclasses, alpha=0.7, s=2, cmap='tab20b')
plt.xlabel("tSNE Dimension-1", size=20)
plt.ylabel("tSNE Dimension-2", size=20)
plt.xticks(size=20)
plt.yticks(size=20)
plt.title("t-SNE Projection of Latent-Space (NSCLC - NCI60)", size=20)

plt.colorbar()
plt.show()

for c_,e_ in enumerate(latent_mus):
  z_test = e_ + latent_sigmas[c_] * np.random.normal(0,1)
  embeddings = z_test.copy()
  tsne = TSNE(n_components=2, init='pca', random_state=0)
  X_tsne = tsne.fit_transform(embeddings)

  plt.figure(figsize=(8, 8))
  plt.scatter(X_tsne[:, 0] , X_tsne[:, 1], c=giclasses, alpha=0.7, s=2, cmap='tab20b')
  plt.xlabel("tSNE Dimension-1", size=20)
  plt.ylabel("tSNE Dimension-2", size=20)
  plt.xticks(size=20)
  plt.yticks(size=20)
  plt.title("t-SNE Projection of Latent-Space (NSCLC - NCI60) _ Epoch: "+str(50*c_), size=20)

  plt.colorbar()
  plt.savefig('tSNElatent_'+str(c_)+'.png', dpi=300)
  plt.show()

c_



"""# **Searching latent space**"""

with tf.Session() as sess:
   

        sess.run(tf.global_variables_initializer(), feed_dict={keep_prob: 0.9})

        for epoch in range(n_epochs):

            # Random shuffling
            np.random.shuffle(train_total_data)
            train_data_ = train_total_data[:, :-NUM_LABELS]
            # 0-1 to 0.001 - 0.999
            train_data_[train_data_[:] == 0] = 0.001
            train_data_[train_data_[:] == 1] = 0.999

            train_labels_ = train_total_data[:, -NUM_LABELS:]

            # Loop over all batches
            print("total_batch : ", str(total_batch))
            for i in range(total_batch):
                offset = (i * batch_size) % (n_samples)
                batch_xs_input = np.array(train_data_[offset:(offset + batch_size), :]).astype(np.float32)
                batch_ys_input = np.array(train_labels_[offset:(offset + batch_size)]).astype(np.float32)
                batch_xs_target = (batch_xs_input).astype(np.float32)

                # add salt & pepper noise
                if True:
                    batch_xs_input = batch_xs_input * np.random.randint(2, size=batch_xs_input.shape)
                    batch_xs_input += np.random.randint(2, size=batch_xs_input.shape)

                _, tot_loss, loss_likelihood, loss_divergence = sess.run(
                    (train_op, loss, neg_marginal_likelihood, KL_divergence),
                    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, y: batch_ys_input, keep_prob: np.array(0.9).astype(np.float32)})
                
            # print cost every epoch
            print("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f" % (
                epoch, tot_loss, loss_likelihood, loss_divergence))
            with open(RESULTS_DIR + '/log.txt', 'a') as epoch_log:
                epoch_log.write("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f \n" % (
                    epoch, tot_loss, loss_likelihood, loss_divergence))

            if epoch%50 == 0:
                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                latent_mus.append(mu_test)
                latent_sigmas.append(sigma_test)

            # if minimum loss is updated or final epoch, make results
            if min_tot_loss > tot_loss or epoch + 1 == n_epochs:
                min_tot_loss = tot_loss

                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                # Plot for analogical reasoning result
                if epoch > 5:
                    # onehot_label= [0:-3<<-2, 1:-2<<-1, 2:-1<<0, 3:0<<1, 4:1<<2, 5:2<<3, 6:3<<4, 7:4<<5, 8:5<<6, 9:6<<7, 10:7<<8, 11:8<<9, 12:9<<10, 13:10<<11, 14:11<<12, 15:12<<13, 16: 13<<14]

                    z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.
                    conditions_to_generate = np.zeros(shape=[NUM_TO_GENERATE, NUM_LABELS])
                    for i in range(NUM_TO_GENERATE):
                        label = i % NUM_LABELS
                        conditions_to_generate[i, label] = 1.0
                    x_output = sess.run(decoded, feed_dict={z_in: z,
                                                            condition_in: conditions_to_generate,
                                                            keep_prob: 1})
                    print('info: x_vector.shape -> {}'.format(x_output.shape))

                    x_output_bit = np.where(x_output > 0.5, 1, 0)

                    """ generate fingerprint """
                    # onehot_label = args.target_label+5
                    condition_gi = [4, 5, 6, 7, 8]
                    for i in condition_gi:
                        dec_fp = open(RESULTS_DIR + '/dec_fp_v' + str(i) + '_e%d.txt' % epoch, 'a')
                        for j, fp in enumerate(x_output_bit.tolist()):
                   
                   
                            fp_str = ""
                            if j % 18 == i + 3 + 1:
                                for bit in fp:
                                    fp_str += str(bit)
                                dec_fp.write(fp_str + ',%f' % i)
                                dec_fp.write('\n')
                        dec_fp.close()
            #if epoch+1==n_epochs:













test_data_ = test_data[:, :-NUM_LABELS].copy()
test_labels_ = test_data[:, -NUM_LABELS:].copy()
example_idx = np.random.choice(range(len(test_data_)),3000)
example = test_data_[example_idx].copy()
example_labels = test_labels_[example_idx].copy()

gaussian_MLP_conditional_encoder(x=example, y=example_labels, n_hidden=500, n_output=20, keep_prob=1)



"""# ***NCI60 Data Mining***"""

import os

os.chdir('/content/results_cvae')
os.mkdir('NCI60')
os.chdir('/content/results_cvae/NCI60')
! wget https://wiki.nci.nih.gov/download/attachments/147193864/GI50.zip

! unzip GI50.zip

gi = pd.read_csv('GI50.csv')
gi

gi['AVERAGE'].max()

! cp /content/drive/MyDrive/maccs.txt /content/maccs.txt

nci = pd.read_csv('/content/maccs.txt')
nci

nci['NLOGGI50'].min()

ncic = nci[['NSC','FP']].copy()

maccs_fp = [x.replace(',','')[1:] for x in ncic['FP']]

import chemfp
import chemfp.encodings
#hex_fps = [chemfp.encodings.from_binary_lsb(x)[1].hex() for x in maccs_fp]

hex_df = pd.DataFrame({'Hex':hex_fps,'Number':range(1,len(hex_fps)+1)})
hex_df.to_csv('hex_df.csv')

nci_fps = pd.read_csv('/content/nci.fps.txt',sep='\t')
nci_fps

! cp /content/ncifps.fps /content/drive/MyDrive/

! cp /content/drive/MyDrive/ncifps.fps /content/

out = []
arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(maccs_fp[763])[1], arena, 0.75).get_ids_and_scores())

out

"""# ***Searching new molecules among NCI***"""

import chemfp
import chemfp.encodings
import matplotlib.pyplot as plt

import numpy

# This project is extended and a library called PyGAD is released to build the genetic algorithm.
# PyGAD documentation: https://pygad.readthedocs.io
# Install PyGAD: pip install pygad
# PyGAD source code at GitHub: https://github.com/ahmedfgad/GeneticAlgorithmPython

def cal_pop_fitness(equation_inputs, pop):
    # Calculating the fitness value of each solution in the current population.
    # The fitness function caulcuates the sum of products between each input and its corresponding weight.
    fitness = numpy.sum(pop*equation_inputs, axis=1)
    return fitness

def select_mating_pool(pop, fitness, num_parents):
    # Selecting the best individuals in the current generation as parents for producing the offspring of the next generation.
    parents = numpy.empty((num_parents, pop.shape[1]))
    for parent_num in range(num_parents):
        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))
        max_fitness_idx = max_fitness_idx[0][0]
        parents[parent_num, :] = pop[max_fitness_idx, :]
        fitness[max_fitness_idx] = -99999999999
    return parents

def crossover(parents, offspring_size):
    offspring = numpy.empty(offspring_size)
    # The point at which crossover takes place between two parents. Usually it is at the center.
    crossover_point = numpy.uint8(offspring_size[1]/2)

    for k in range(offspring_size[0]):
        # Index of the first parent to mate.
        parent1_idx = k%parents.shape[0]
        # Index of the second parent to mate.
        parent2_idx = (k+1)%parents.shape[0]
        # The new offspring will have its first half of its genes taken from the first parent.
        offspring[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]
        # The new offspring will have its second half of its genes taken from the second parent.
        offspring[k, crossover_point:] = parents[parent2_idx, crossover_point:]
    return offspring

def mutation(offspring_crossover):
    # Mutation changes a single gene in each offspring randomly.
    for idx in range(offspring_crossover.shape[0]):
        # The random value to be added to the gene.
        random_value = numpy.random.uniform(-5.0, 5.0, 1)
        offspring_crossover[idx, 4] = offspring_crossover[idx, 4] + random_value
    return offspring_crossover



def fitness(z):
    for i in range(NUM_TO_GENERATE):
        label = i % NUM_LABELS
        conditions_to_generate[i, label] = 1.0
        x_output = sess.run(decoded, feed_dict={z_in: z,
                                                condition_in: conditions_to_generate,
                                                keep_prob: 1})
        x_output_bit = np.where(x_output > 0.5, 1, 0)
    gen_fp = [''.join([c for c in x_output_bit[x,:].astype('str')]) for x in range(0,x_output_bit.shape[0])]
    sim_fp = []
    out = []
    arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
    out = []
    fits = []
    for f in gen_fp:
        out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.75).get_ids_and_scores())
        sim_fp.append(out)
        sorted_by_second = sorted(sim_fp[-1], key=lambda tup: tup[1], reverse=True)
        top_gi = nci.loc[[int(x[0]) for x in sorted_by_second][0],'NLOGGI50']
        fits.append(top_gi)
    return(fits)

with tf.Session() as sess:

        sess.run(tf.global_variables_initializer(), feed_dict={keep_prob: 0.9})

        for epoch in range(n_epochs):

            # Random shuffling
            np.random.shuffle(train_total_data)
            train_data_ = train_total_data[:, :-NUM_LABELS]
            # 0-1 to 0.001 - 0.999
            train_data_[train_data_[:] == 0] = 0.001
            train_data_[train_data_[:] == 1] = 0.999

            train_labels_ = train_total_data[:, -NUM_LABELS:]

            # Loop over all batches
            print("total_batch : ", str(total_batch))
            for i in range(total_batch):
                offset = (i * batch_size) % (n_samples)
                batch_xs_input = np.array(train_data_[offset:(offset + batch_size), :]).astype(np.float32)
                batch_ys_input = np.array(train_labels_[offset:(offset + batch_size)]).astype(np.float32)
                batch_xs_target = (batch_xs_input).astype(np.float32)

                # add salt & pepper noise
                if True:
                    batch_xs_input = batch_xs_input * np.random.randint(2, size=batch_xs_input.shape)
                    batch_xs_input += np.random.randint(2, size=batch_xs_input.shape)

                _, tot_loss, loss_likelihood, loss_divergence = sess.run(
                    (train_op, loss, neg_marginal_likelihood, KL_divergence),
                    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, y: batch_ys_input, keep_prob: np.array(0.9).astype(np.float32)})
                
            # print cost every epoch
            print("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f" % (
                epoch, tot_loss, loss_likelihood, loss_divergence))
            with open(RESULTS_DIR + '/log.txt', 'a') as epoch_log:
                epoch_log.write("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f \n" % (
                    epoch, tot_loss, loss_likelihood, loss_divergence))

            if epoch+1==n_epochs:
                # onehot_label= [0:-3<<-2, 1:-2<<-1, 2:-1<<0, 3:0<<1, 4:1<<2, 5:2<<3, 6:3<<4, 7:4<<5, 8:5<<6, 9:6<<7, 10:7<<8, 11:8<<9, 12:9<<10, 13:10<<11, 14:11<<12, 15:12<<13, 16: 13<<14]
                conditions_to_generate = np.zeros(shape=[NUM_TO_GENERATE, NUM_LABELS])
                pops, pops_bit, pops_fit = [],[],[]
                z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.
                new_population = z.copy()
                num_generations = 200
                num_parents_mating = 10
                new_new = new_population.copy()
                for generation in range(num_generations):
                    print('Generation : ', generation)
                    new_population = new_new.copy()
                    # First Evaluation of Fitness
                    for i in range(NUM_TO_GENERATE):
                        label = i % NUM_LABELS
                        conditions_to_generate[i, label] = 1.0
                    x_output = sess.run(decoded, feed_dict={z_in: new_population,
                                                            condition_in: conditions_to_generate,
                                                            keep_prob: 1})
                    x_output_bit = np.where(x_output > 0.5, 1, 0)
                    gen_fp = [''.join([c for c in x_output_bit[x,:].astype('str')]) for x in range(0,x_output_bit.shape[0])].copy()
                    arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
                    sim_fp = []
                    out = []
                    fits = []
                    for f in gen_fp:
                        out = chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.8).get_ids_and_scores()
                        if len(out)==0:
                            continue
                        sim_fp = out.copy()
                        sorted_by_second = sorted(sim_fp, key=lambda tup: tup[1], reverse=True)
                        top_gi = float(nci.loc[[int(x[0])-1 for x in sorted_by_second],'NLOGGI50'].max())+5
                        fits.append(top_gi)
                    fitness = fits.copy()
                    if len(fitness)==0:
                        continue

                    # Selecting Parents
                    #parents = select_mating_pool(new_population,fitness,num_parents_mating)
                    parents = numpy.empty((num_parents_mating, new_population.shape[1]))
                    sorted_fitness = fitness.copy()
                    sorted_fitness.sort()

                    for parent_num in range(num_parents_mating):
                        max_fitness_idx = numpy.where(fitness == numpy.max(fitness))
                        max_fitness_idx = max_fitness_idx[0][0]
                        parents[parent_num, :] = new_population[max_fitness_idx, :]
                        fitness[max_fitness_idx] = -99999999999

                    # Crossover
                    #offspring_crossover = crossover(parents,offspring_size=(z.shape[0]-parents.shape[0], dim_z))
                    offspring_size = (z.shape[0]-parents.shape[0], dim_z)
                    offspring_crossover = numpy.empty(offspring_size)
                    crossover_point = numpy.uint8(offspring_size[1]/2)*2+1
                    for k in range(offspring_size[0]):
                        # Index of the first parent to mate.
                        parent1_idx = k%parents.shape[0]
                        # Index of the second parent to mate.
                        parent2_idx = (k+1)%parents.shape[0]
                        # The new offspring will have its first half of its genes taken from the first parent.
                        offspring_crossover[k, 0:crossover_point] = parents[parent1_idx, 0:crossover_point]
                        # The new offspring will have its second half of its genes taken from the second parent.
                        offspring_crossover[k, crossover_point:] = parents[parent2_idx, crossover_point:]

                    # Mutation
                    #offspring_mutation = mutation(offspring_crossover)
                    offspring_mutation = offspring_crossover.copy()
                    for idx in range(offspring_crossover.shape[0]):
                        # The random value to be added to the gene.                      
                        for d_ in range(offspring_crossover.shape[1]):
                            random_value = numpy.random.randn()
                            offspring_mutation[idx, :] = offspring_crossover[idx, :] + random_value
                    
                    new_new[0:parents.shape[0], :] = parents.copy()
                    new_new[parents.shape[0]:, :] = offspring_mutation.copy()

                    #for i in range(NUM_TO_GENERATE):
                    #    label = i % NUM_LABELS
                    #    conditions_to_generate[i, label] = 1.0
                    #x_output = sess.run(decoded, feed_dict={z_in: new_new,
                    #                                        condition_in: conditions_to_generate,
                    #                                        keep_prob: 1})
                    #x_output_bit = np.where(x_output > 0.5, 1, 0)
                    #gen_fp = [''.join([c for c in x_output_bit[x,:].astype('str')]) for x in range(0,x_output_bit.shape[0])]
                    #arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
                    #sim_fp = []
                    #out = []
                    #fits = []
                    #for f in gen_fp:
                    #    if len(out)==0:
                    #        break
                    #    out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.8).get_ids_and_scores())
                    #    sim_fp.append(out)
                    #    sorted_by_second = sorted(sim_fp[-1], key=lambda tup: tup[1], reverse=True)
                    #    top_gi = float(nci.loc[[int(x[0])-1 for x in sorted_by_second],'NLOGGI50'].max())+5
                    #    fits.append(top_gi)
                    
                    pops.append(new_population)
                    pops_bit.append(x_output_bit)
                    pops_fit.append(fits)
                    #fitness = fits.copy()
                    #if len(fitness)==0:
                    #    continue
                    print("Best result : ", numpy.max(fitness))

                best_match_idx = numpy.where(fitness == numpy.max(fitness))
                print("Best solution : ", new_population[best_match_idx, :])
                print("Best solution fitness : ", fitness[best_match_idx])

                #for i in range(NUM_LABELS):
                #    label = i % NUM_LABELS
                #    conditions_to_generate[i, label] = 1.0
                #x_output = sess.run(decoded, feed_dict={z_in: new_population[best_match_idx,:].reshape((-1,20)),
                                                        #condition_in: conditions_to_generate,
                                                        #keep_prob: 1})
                #print('info: x_vector.shape -> {}'.format(x_output.shape))

                #x_output_bit_final = np.where(x_output > 0.5, 1, 0)

                #print(x_output_bit_final)

fitness

plt.matshow(pops[-5])w
plt.matshow(pops_bit[-5])ص

gen1 = ''.join([str(x) for x in list(pops_bit[-1][0,:])])
gen50 = ''.join([str(x) for x in list(pops_bit[-50][0,:])])
gen100 = ''.join([str(x) for x in list(pops_bit[-100][0,:])])

plt.matshow(pops_bit[55][-1,:].reshape((1,-1)))
plt.matshow(pops_bit[-1][-1,:].reshape((1,-1)))

simsim = chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(gen1)[1], arena, 0.8).get_ids_and_scores()
sorted_by_second = sorted(simsim, key=lambda tup: tup[1], reverse=True)
nci.loc[[int(x[0])-1 for x in sorted_by_second],'NLOGGI50']#.max()+5

gen_fp = [''.join([c for c in pops_bit[0][x,:].astype('str')]) for x in range(0,pops_bit[0].shape[0])]
for f in gen_fp:
    out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.8).get_ids_and_scores())
                        if len(out)==0:
                            continue
                        sim_fp.append(out)
                        sorted_by_second = sorted(sim_fp[-1], key=lambda tup: tup[1], reverse=True)
                        top_gi = float(nci.loc[[int(x[0])-1 for x in sorted_by_second],'NLOGGI50'].max())+5
                        fits.append(top_gi)
                    fitness = fits.copy()

plt.matshow(pops[-2])
plt.matshow(parents)
plt.matshow(offspring_crossover)
plt.matshow(offspring_mutation)
plt.matshow(pops[-1])

plt.matshow(pops_bit[-2])
plt.matshow(pops_bit[-1])

gen_fp = [''.join([c for c in pops_bit[-1][x,:].astype('str')]) for x in range(0,pops_bit[-1].shape[0])]

f = gen_fp[20]

sorted_by_second = sorted(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.5).get_ids_and_scores(), key=lambda tup: tup[1], reverse=True)
sorted_by_second

nci.loc[26305,'NLOGGI50']

tests = np.concatenate((np.array([int(x) for x in list(nci.loc[1989,'FP'].replace(',','')[1:])]).reshape((1,-1)),pops_bit[-2][17].reshape((1,-1))))
plt.matshow(tests)
#plt.matshow(np.array([int(x) for x in list(nci.loc[28915,'FP'].replace(',',''))]).reshape((1,-1)))
#plt.matshow(pops_bit[-2][17].reshape((1,-1)))

nci.replace(',','')

plt.matshow(pops[0])
plt.matshow(pops[1])
plt.matshow(pops[2])
plt.matshow(pops_bit[0])
plt.matshow(pops_bit[1])
plt.matshow(pops_bit[2])

plt.matshow(pops[0])
plt.matshow(parents)

plt.plot(np.array([max(x) for x in pops_fit]))

plt.plot(pops_fit[0])
plt.plot(pops_fit[1])
plt.plot(pops_fit[2])
plt.plot(pops_fit[3])

plt.matshow(offspring_crossover)

for m in pops_bit:
  plt.plot(pops_fit)

gen_fp = [''.join([c for c in x_output_bit[x,:].astype('str')]) for x in range(0,x_output_bit.shape[0])]
arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
sim_fp = []
out = []
fits = []
for f in gen_fp:
    out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.75).get_ids_and_scores())
    sim_fp.append(out)
    sorted_by_second = sorted(sim_fp[-1], key=lambda tup: tup[1], reverse=True)
    top_gi = nci.loc[[int(x[0]) for x in sorted_by_second][0],'NLOGGI50']
    fits.append(top_gi)
                    
fitness = fits

fitness

import matplotlib.pyplot as plt

"""# ***Firefly Algorithm (Trust)***"""

! pip install deap

import numpy as np

from scipy import optimize
from deap.benchmarks import schwefel

from abc import ABCMeta
from abc import abstractmethod
from six import add_metaclass


@add_metaclass(ABCMeta)
class ObjectiveFunction(object):

    def __init__(self, name, dim, minf, maxf):
        self.name = name
        self.dim = dim
        self.minf = minf
        self.maxf = maxf

    def sample(self):
        return np.random.uniform(low=self.minf, high=self.maxf, size=self.dim)

    def custom_sample(self):
        return np.repeat(self.minf, repeats=self.dim) \
               + np.random.uniform(low=0, high=1, size=self.dim) *\
               np.repeat(self.maxf - self.minf, repeats=self.dim)

    @abstractmethod
    def evaluate(self, x):
        pass

class Fitness(ObjectiveFunction):
    def __init__(self, dim):
        super(Fitness, self).__init__('Fitness', dim, -5, 5)

    def evaluate(self, x):
        x = x.reshape((-1,self.dim))
        return fitness(x)

import numpy as np

from copy import deepcopy
from abc import ABCMeta
from six import add_metaclass


@add_metaclass(ABCMeta)
class ArtificialBee(object):

    TRIAL_INITIAL_DEFAULT_VALUE = 0
    INITIAL_DEFAULT_PROBABILITY = 0.0

    def __init__(self, obj_function):
        self.pos = obj_function.custom_sample()
        self.obj_function = obj_function
        self.minf = obj_function.minf
        self.maxf = obj_function.maxf
        self.fitness = obj_function.evaluate(self.pos)
        self.trial = ArtificialBee.TRIAL_INITIAL_DEFAULT_VALUE
        self.prob = ArtificialBee.INITIAL_DEFAULT_PROBABILITY

    def evaluate_boundaries(self, pos):
        if (pos < self.minf).any() or (pos > self.maxf).any():
            pos[pos > self.maxf] = self.maxf
            pos[pos < self.minf] = self.minf
        return pos

    def update_bee(self, pos, fitness):
        self.pos = pos
        self.fitness = fitness
        self.trial = 0
        if fitness <= self.fitness:
            self.pos = pos
            self.fitness = fitness
            self.trial = 0
        else:
            self.trial += 1

    def reset_bee(self, max_trials):
        if self.trial >= max_trials:
            self.__reset_bee()

    def __reset_bee(self):
        self.pos = self.obj_function.custom_sample()
        self.fitness = self.obj_function.evaluate(self.pos)
        self.trial = ArtificialBee.TRIAL_INITIAL_DEFAULT_VALUE
        self.prob = ArtificialBee.INITIAL_DEFAULT_PROBABILITY

class EmployeeBee(ArtificialBee):

    def explore(self, max_trials):
        if self.trial <= max_trials:
            component = np.random.choice(self.pos)
            phi = np.random.uniform(low=-1, high=1, size=len(self.pos))
            n_pos = self.pos + (self.pos - component) * phi
            n_pos = self.evaluate_boundaries(n_pos)
            n_fitness = self.obj_function.evaluate(n_pos)
            self.update_bee(n_pos, n_fitness)

    def get_fitness(self):
        return 1 / (1 + self.fitness) if self.fitness >= 0 else 1 + np.abs(self.fitness)

    def compute_prob(self, max_fitness):
        self.prob = self.get_fitness() / max_fitness

class OnLookerBee(ArtificialBee):

    def onlook(self, best_food_sources, max_trials):
        candidate = np.random.choice(best_food_sources)
        self.__exploit(candidate.pos, candidate.fitness, max_trials)

    def __exploit(self, candidate, fitness, max_trials):
        if self.trial <= max_trials:
            component = np.random.choice(candidate)
            phi = np.random.uniform(low=-1, high=1, size=len(candidate))
            n_pos = candidate + (candidate - component) * phi
            n_pos = self.evaluate_boundaries(n_pos)
            n_fitness = self.obj_function.evaluate(n_pos)

            if n_fitness <= fitness:
                self.pos = n_pos
                self.fitness = n_fitness
                self.trial = 0
            else:
                self.trial += 1

class ABC(object):

    def __init__(self, obj_function, colony_size=30, n_iter=5000, max_trials=100):
        self.colony_size = colony_size
        self.obj_function = obj_function

        self.n_iter = n_iter
        self.max_trials = max_trials

        self.optimal_solution = None
        self.optimality_tracking = []

    def __reset_algorithm(self):
        self.optimal_solution = None
        self.optimality_tracking = []

    def __update_optimality_tracking(self):
        self.optimality_tracking.append(self.optimal_solution.fitness)

    def __update_optimal_solution(self):
        n_optimal_solution = min(self.onlokeer_bees + self.employee_bees,
                                 key=lambda bee: bee.fitness)
        if not self.optimal_solution:
            self.optimal_solution = deepcopy(n_optimal_solution)
        else:
            if n_optimal_solution.fitness < self.optimal_solution.fitness:
                self.optimal_solution = deepcopy(n_optimal_solution)

    def __initialize_employees(self):
        self.employee_bees = []
        for itr in range(self.colony_size // 2):
            self.employee_bees.append(EmployeeBee(self.obj_function))

    def __initialize_onlookers(self):
        self.onlokeer_bees = []
        for itr in range(self.colony_size // 2):
            self.onlokeer_bees.append(OnLookerBee(self.obj_function))

    def __employee_bees_phase(self):
        for self_ in self.employee_bees:
          if self_.trial <= self.max_trials:
              component = np.random.choice(self_.pos)
              phi = np.random.uniform(low=-1, high=1, size=len(self_.pos))
              n_pos = self_.pos + (self_.pos - component) * phi
              n_pos = self_.evaluate_boundaries(n_pos)
              n_fitness = self_.obj_function.evaluate(n_pos)
              self_.update_bee(n_pos, n_fitness)
        #map(lambda bee: bee.explore(self.max_trials), self.employee_bess)
    
    def __calculate_probabilities(self):
        fitness_list = []
        for bee in self.employee_bees:
          if bee.fitness >= 0:
            fitness_list.append(1/(1+bee.fitness))
          else:
            fitness_list.append(1+np.abs(bee.fitness))
        sum_fitness = sum(fitness_list)
        for bee in self.employee_bees:
          if bee.fitness >= 0:
            bee.prob = (1/(1+bee.fitness)) / sum_fitness
          else:
            bee.prob = (1+np.abs(bee.fitness)) / sum_fitness
        #map(lambda bee: bee.compute_prob(sum_fitness), self.employee_bees)

    def __select_best_food_sources(self):
        self.best_food_sources = []
        while len(self.best_food_sources)==0:
          threshold = np.random.uniform(low=0, high=1)
          self.best_food_sources = [x for x in self.employee_bees if x.prob>threshold]

    def __onlooker_bees_phase(self):
        #map(lambda bee: bee.onlook(self.best_food_sources, self.max_trials),self.onlokeer_bees)
        for bee in self.onlokeer_bees:
          candidate = np.random.choice(self.best_food_sources)
          if bee.trial <= self.max_trials:
            component = np.random.choice(candidate.pos)
            phi = np.random.uniform(low=-1, high=1, size=len(candidate.pos))
            n_pos = candidate.pos + (candidate.pos - component) * phi
            n_pos = bee.evaluate_boundaries(n_pos)
            n_fitness = bee.obj_function.evaluate(n_pos)

            if n_fitness <= bee.fitness:
              bee.pos = n_pos
              bee.fitness = n_fitness
              bee.trial = 0
            

    def __scout_bees_phase(self):
        map(lambda bee: bee.reset_bee(self.max_trials),
            self.onlokeer_bees + self.employee_bees)

    def optimize(self):
        self.__reset_algorithm()
        self.__initialize_employees()
        self.__initialize_onlookers()
        for itr in range(self.n_iter):
            self.__employee_bees_phase()
            self.__update_optimal_solution()

            self.__calculate_probabilities()
            self.__select_best_food_sources()
            self.__onlooker_bees_phase()
            self.__scout_bees_phase()

            self.__update_optimal_solution()
            self.__update_optimality_tracking()
            print("iter: {} = cost: {}"
                  .format(itr, "%04.03e" % self.optimal_solution.fitness))

import numpy as np
import matplotlib.pyplot as plt

#from algorithm.abc import ABC

from matplotlib.style import use

#from ObjectiveFunction import Rastrigin
#from objection_function import Rosenbrock
#from objection_function import Sphere
#from objection_function import Schwefel


use('classic')


def get_objective(objective, dimension=20):
    objectives = {'Fitness':Fitness(dimension)}
    return objectives[objective]


def simulate(obj_function, colony_size=50, n_iter=5000,
             max_trials=100, simulations=10):
    itr = range(n_iter)
    values = np.zeros(n_iter)
    box_optimal = []
    for _ in range(simulations):
        optimizer = ABC(obj_function=get_objective(obj_function),
                        colony_size=colony_size, n_iter=n_iter,
                        max_trials=max_trials)
        optimizer.optimize()
        values += np.array(optimizer.optimality_tracking)
        box_optimal.append(optimizer.optimal_solution.fitness)
        print(optimizer.optimal_solution.pos)
        print(optimizer.optimal_solution.fitness)
    values /= simulations
    print(list(np.where(values == min(values))[0])[0])
    plt.plot(itr, values, lw=0.5, label=obj_function)
    plt.legend(loc='upper right')


def main():
    plt.figure(figsize=(10, 7))
    simulate('Fitness')
    plt.ticklabel_format(axis='y', style='sci', scilimits=(-2, 2))
    plt.xticks(rotation=45)
    plt.show()







gen_fp = [''.join([c for c in x_output_bit[x,:].astype('str')]) for x in range(0,x_output_bit.shape[0])]

sim_fp = []
out = []
arena = chemfp.load_fingerprints('/content/ncifps.fps', reorder=False, format="fps")
out = []
for f in gen_fp:
    out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(f)[1], arena, 0.75).get_ids_and_scores())
    sim_fp.append(out)

sorted_by_second = sorted(sim_fp[0], key=lambda tup: tup[1], reverse=True)
nci.loc[[int(x[0]) for x in sorted_by_second][0],'NLOGGI50']

z

def fitness(z):

plt.bar(x=range(len(nci.loc[[int(x[0]) for x in out],'NLOGGI50'])),height=nci.loc[[int(x[0]) for x in out],'NLOGGI50'])



import os

"""# **Similarity Search using PubChem**"""

os.chdir('/content/results_cvae/')
os.mkdir('fps')
os.chdir('/content/drive/MyDrive/')
! cp *.zip /content/results_cvae/fps/

os.chdir('/content/results_cvae/fps/')
for f_ in [x for x in os.listdir() if x.endswith('.zip') and x.startswith('fps')]:
  ! echo {f_}
  ! unzip {f_}

! mv /content/results_cvae/fps/fps/*.fps /content/results_cvae/fps/

import chemfp
import chemfp.encodings
import chemfp.bitops
import rdkit
from rdkit import Chem
from rdkit import DataStructs
from rdkit.Chem import MACCSkeys
from rdkit.Chem.Fingerprints import FingerprintMols
from rdkit.Chem import Draw
import time
import pandas as pd
import os
from numpy.random import randn

# Required installation : chemfp (python 2.7)
# installed in jupyter virtual environment : source activate chemfp

def search_pubchem(MACCS_bit, thr):
    """
    search
    """
    out=[]
    converted=chemfp.encodings.from_binary_lsb(MACCS_bit)
    for i in range(6400):
        a="/content/results_cvae/fps/Compound_"+str(i*25000+1).zfill(9) + '_' + str((i+1)*25000).zfill(9) +".fps"
        try:
            arena = chemfp.load_fingerprints(a, reorder=False, format="fps")
            out.extend(chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, thr).get_ids_and_scores())
        except:
            i=i
            #print ("No such file or directory: " + a)
    return out

def fp_eval_pubchem(filename):
    j=0
    f = open("./"+filename+'.txt', 'r')
    #os.mkdir(filename)
    idx=[]
    num_th_80=[]
    num_th_85 =[]
    num_th_90 =[]
    num_th_95 =[]
    num_th_98 =[]


    while True:
        if j==300:
            break
        j += 1
        line = f.readline()
        if not line: break
        now = line.split(",")
        print(len(now[0]),now[0])
        start_time = time.time()
        res = search_pubchem(now[0], thr=0.8)
        print ("%d: " % (j))
        print ("number of similar compound : %d" % (len(res)))
        print ("--- %s seconds ---" % (time.time() - start_time))
        idx.append(j)

        cid_array = []
        sim_array = []
        for id in res:
            cid_array += [id[0]]
            sim_array += [id[1]]
        cid_array = [int(i) for i in cid_array]
        result = {'CID': cid_array, 'similarity': sim_array}
        a = pd.DataFrame(result)
        a = a.sort_values(by='similarity', ascending=False)
        a.to_csv(filename+"_result_pubchem_" + str(j) + '.csv', index=False)

        num_th_80.append(a['CID'].describe()[0])

        a_over_85 = a.loc[(a['similarity'] > 0.85)]
        num_th_85.append(a_over_85['CID'].describe()[0])

        a_over_90 = a.loc[(a['similarity'] > 0.90)]
        num_th_90.append(a_over_90['CID'].describe()[0])

        a_over_95 = a.loc[(a['similarity'] > 0.95)]
        num_th_95.append(a_over_95['CID'].describe()[0])

        a_over_98 = a.loc[(a['similarity'] > 0.98)]
        num_th_98.append(a_over_98['CID'].describe()[0])

    eval_result = {'count >=0.8': num_th_80, 'count >=0.85': num_th_85, 'count >=0.9': num_th_90, 'count >=0.95': num_th_95, 'count >=0.98': num_th_98}
    df = pd.DataFrame(eval_result, columns=['count >=0.8','count >=0.85','count >=0.9','count >=0.95','count >=0.98'])
    df.to_csv(filename + '_eval.csv', index=False)
    with open(filename + '_log.txt', 'a') as result_log:
        a8=df.loc[(df['count >=0.8'] > 0)]
        a85 = df.loc[(df['count >=0.85'] > 0)]
        a9 = df.loc[(df['count >=0.9'] > 0)]
        a95 = df.loc[(df['count >=0.95'] > 0)]
        a98 = df.loc[(df['count >=0.98'] > 0)]

        result_log.write("number of fingerprint on DB(>=0.8) : %f\n" % (a8['count >=0.8'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.85) : %f\n" % (a85['count >=0.85'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.9) : %f\n" % (a9['count >=0.9'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.95) : %f\n" % (a95['count >=0.95'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.98) : %f\n" % (a98['count >=0.98'].describe()[0]))
    f.close()
epoch = "9"
#fp_eval_pubchem("dec_fp_v2_e"+epoch)
#fp_eval_pubchem("dec_fp_v4_e"+epoch)
#fp_eval_pubchem("dec_fp_v6_e"+epoch)
#fp_eval_pubchem("dec_fp_v8_e"+epoch)
#fp_eval_pubchem("dec_fp_v10_e"+epoch)

os.chdir('/content/results_cvae')

fp_eval_pubchem("dec_fp_v8_e49")

Mcfp = [''.join([str(x) for x in list(pops_bit[-1][y,:])])+',8.000000' for y in range(pops_bit[-1].shape[0])]
for m in Mcfp:
  print(m)

plt.plot([max(x) for x in pops_fit])

len(fpfp)

for i in range(6000):
    a="/content/results_cvae/fps/Compound_"+str(i*25000+1).zfill(9) + '_' + str((i+1)*25000).zfill(9) +".fps"
    try:
        arena = chemfp.load_fingerprints(a, reorder=False, format="fps")
        out.extend(chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(fpfp)[1], arena, 0.5).get_ids_and_scores())
    except:
        i=i
        #print ("No such file or directory: " + a)

out

import chemfp
import chemfp.encodings

arena = chemfp.load_fingerprints('/content/results_cvae/fps/Compound_000000001_000025000.fps', format='fps')
chemfp.search.threshold_tanimoto_search_fp(chemfp.encodings.from_binary_lsb(fpfp)[1], arena, 0.5).get_ids_and_scores()

chemfp.encodings.from_binary_lsb('0000002000000100000230510888042688189af709')[1]

nci_df = pd.DataFrame([x.hex() for x in nci_bits])
nci_df.to_csv('ncidf.csv')

data[data==0.001] = 0
data[data==0.999] = 1
data = data.astype('int32')

data_fps = data.iloc[:,:166]
cols = data_fps.columns
maccs_fps = data_fps[cols].apply(lambda row: ''.join(row.values.astype(str)), axis=1)

nci_bits = [chemfp.encodings.from_binary_lsb(x)[1] for x in maccs_fps]

chemfp.encodings.from_binary_lsb(maccs_fps[0])[1]

def get_input_gi50_data(gi50_data_file_name, test_div=10, normalize=False, label = 18):

  data = pd.read_csv('maccs.txt', on_bad_lines='skip')
  data = data[['NSC','NLOGGI50','FP']]
  mols = data.copy()
  data = np.concatenate((np.array([[int(x) for x in data.loc[y,'FP'].split(',')] for y in range(len(data))]),np.array(data[['NLOGGI50']])),axis=1)
  data = data.astype(np.float32)
  data_maccs = data[:,0]
  data[:,-1] = np.round(data[:,-1].astype(np.int32))
  data[:,-1] = data[:,-1]+abs(data[:,-1].min())
  data = data.astype(np.int32)
  b = np.zeros((data[:,-1].size, data[:,-1].max()+1))
  b[np.arange(data[:,-1].size),data[:,-1].astype(np.int32)] = 1
  data = np.concatenate((data,b),axis=1)
  data = data.astype(np.float32)
  print('info: get_input_data: (gi50) data.shape -> {}'.format(data.shape))
  np.random.shuffle(data)
  test_size = data.shape[0] // test_div
  label_data = np.copy(data[:,-label:])
  feature = np.copy(data[:,0:-(label)])
  print(data.shape)
  data = np.delete(data,167,axis=1)
  data = np.delete(data,0,axis=1)
  print(data.shape)

  #datadata = np.copy()
  if normalize:
    data[:,-1:] = (data[:,-1:] - data[:,-1:].min(axis=0)) / (data[:,-1:].max(axis=0) - data[:,-1:].min(axis=0))
  
  test_data, train_data = np.vsplit(data, [test_size])
  test_label, train_label = np.vsplit(label_data, [test_size])
  return(test_data,train_data,test_label,train_label)

out = []
arena = chemfp.load_fingerprints(a, reorder=False, format="fps")
out.extend(chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, thr).get_ids_and_scores())

"""# ***Creating New Z Values***"""

with tf.Session() as sess:
   

        sess.run(tf.global_variables_initializer(), feed_dict={keep_prob: 0.9})

        for epoch in range(n_epochs):

            # Random shuffling
            np.random.shuffle(train_total_data)
            train_data_ = train_total_data[:, :-NUM_LABELS]
            # 0-1 to 0.001 - 0.999
            train_data_[train_data_[:] == 0] = 0.001
            train_data_[train_data_[:] == 1] = 0.999

            train_labels_ = train_total_data[:, -NUM_LABELS:]

            # Loop over all batches
            print("total_batch : ", str(total_batch))
            for i in range(total_batch):
                offset = (i * batch_size) % (n_samples)
                batch_xs_input = np.array(train_data_[offset:(offset + batch_size), :]).astype(np.float32)
                batch_ys_input = np.array(train_labels_[offset:(offset + batch_size)]).astype(np.float32)
                batch_xs_target = (batch_xs_input).astype(np.float32)

                # add salt & pepper noise
                if True:
                    batch_xs_input = batch_xs_input * np.random.randint(2, size=batch_xs_input.shape)
                    batch_xs_input += np.random.randint(2, size=batch_xs_input.shape)

                _, tot_loss, loss_likelihood, loss_divergence = sess.run(
                    (train_op, loss, neg_marginal_likelihood, KL_divergence),
                    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, y: batch_ys_input, keep_prob: np.array(0.9).astype(np.float32)})
                
            # print cost every epoch
            print("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f" % (
                epoch, tot_loss, loss_likelihood, loss_divergence))
            with open(RESULTS_DIR + '/log.txt', 'a') as epoch_log:
                epoch_log.write("epoch %d: L_tot %03.2f L_likelihood %03.2f L_divergence %03.2f \n" % (
                    epoch, tot_loss, loss_likelihood, loss_divergence))

            if epoch%50 == 0:
                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                latent_mus.append(mu_test)
                latent_sigmas.append(sigma_test)

            # if minimum loss is updated or final epoch, make results
            if min_tot_loss > tot_loss or epoch + 1 == n_epochs:
                min_tot_loss = tot_loss

                np.random.shuffle(test_data)
                test_data_ = test_data[:, :-NUM_LABELS].copy()
                test_labels_ = test_data[:, -NUM_LABELS:].copy()
                example_idx = np.random.choice(range(len(test_data_)),3000)
                example = test_data_[example_idx]
                example_labels = test_labels_[example_idx]

                batch_xs_input_test = example.astype(np.float32)
                batch_ys_input_test = example_labels.astype(np.float32)
                batch_xs_target_test = (batch_xs_input_test).astype(np.float32)

                
                mu_test, sigma_test = sess.run((encoded_mu, encoded_sigma),
                                               feed_dict={x_hat_: batch_xs_input_test, y_: batch_ys_input_test,
                                                          keep_prob: np.array(0.9).astype(np.float32)}
                                               )
                # Plot for analogical reasoning result
                if epoch > 5:
                    # onehot_label= [0:-3<<-2, 1:-2<<-1, 2:-1<<0, 3:0<<1, 4:1<<2, 5:2<<3, 6:3<<4, 7:4<<5, 8:5<<6, 9:6<<7, 10:7<<8, 11:8<<9, 12:9<<10, 13:10<<11, 14:11<<12, 15:12<<13, 16: 13<<14]

                    z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.
                    conditions_to_generate = np.zeros(shape=[NUM_TO_GENERATE, NUM_LABELS])
                    for i in range(NUM_TO_GENERATE):
                        label = i % NUM_LABELS
                        conditions_to_generate[i, label] = 1.0
                    x_output = sess.run(decoded, feed_dict={z_in: z,
                                                            condition_in: conditions_to_generate,
                                                            keep_prob: 1})
                    print('info: x_vector.shape -> {}'.format(x_output.shape))

                    x_output_bit = np.where(x_output > 0.5, 1, 0)

                    """ generate fingerprint """
                    # onehot_label = args.target_label+5
                    condition_gi = [4, 5, 6, 7, 8]
                    for i in condition_gi:
                        dec_fp = open(RESULTS_DIR + '/dec_fp_v' + str(i) + '_e%d.txt' % epoch, 'a')
                        for j, fp in enumerate(x_output_bit.tolist()):
                            fp_str = ""
                            if j % 18 == i + 3 + 1:
                                for bit in fp:
                                    fp_str += str(bit)
                                dec_fp.write(fp_str + ',%f' % i)
                                dec_fp.write('\n')
                        dec_fp.close()
                if epoch+1==n_epochs:
                    def score(z):
                        conditions_to_generate = np.ones(shape=[NUM_TO_GENERATE, NUM_LABELS])
                        x_output = sess.run(decode, feed_dict={z_in: z, condition_in: conditions_to_generate,
                                                               keep_prob: 1})
                        x_output_bit = np.where(x_output > 0.5, 1, 0)
                        
                    z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.


            #if epoch+1==n_epochs:

z = np.random.randn(NUM_TO_GENERATE, dim_z) * 1.
conditions_to_generate = np.zeros(shape=[NUM_TO_GENERATE, NUM_LABELS])
for i in range(NUM_TO_GENERATE):
  label = i % NUM_LABELS
  conditions_to_generate[i, label] = 1.0

import matplotlib.pyplot as plt
plt.matshow(conditions_to_generate)

"""# **Creating PubChem Database**"""

!wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh
!chmod +x Miniconda3-latest-Linux-x86_64.sh
!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local
!time conda install -q -y -c conda-forge rdkit
!time conda install -q -y -c openbabel openbabel

! python -m pip install chemfp -i https://chemfp.com/packages/

! pip install rdkit-pypi

mfp

chemfp.search.threshold_tanimoto_search_fp(mfp)



import os

! pip install wget

import wget
os.chdir('/content/results_cvae/')
os.mkdir('fps')

os.chdir('/content/results_cvae/fps')

! rm *.gz

# Downloading PubChem from ftp in 500K Chunks 
dir = '/fps/'
i=-1
while True:
    try:
      i = i + 1
      url = 'https://ftp.ncbi.nih.gov/pubchem/Compound/CURRENT-Full/SDF/Compound_'
      url = url + str(i*500000+1).zfill(9) + '_' + str((i+1)*500000).zfill(9) +'.sdf.gz'
      filename = wget.download(url)
    except:
      continue
    if i==3:
      break

! gunzip *.sdf.gz
! rm *.sdf.gz

bigsdfs = [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf')].copy()
j=0
for f in bigsdfs:
    split_number= 25000
    number_of_sdfs = split_number
    i=0
    name = 'Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf'
    f2=open(name,'w')
    for line in open(f+'.sdf'):
	    f2.write(line)
	    if line[:4] == "$$$$":
		    i+=1
	    if i > number_of_sdfs:
		    number_of_sdfs += split_number 
		    f2.close()
		    j+=1
		    f2=open('Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf','w')
    print(i)



for f_r in [x for x in os.listdir() if x not in bigsdfs]:
    if f_r == '.ipynb_checkpoints':
      continue
    if f_r == 'fps':
      continue  
    os.remove(f_r)

! rdkit2fps --maccs166 Compound_000000001_000025000.sdf -o nono.fps

import subprocess
for f_ in [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf')]:
    bashCommand = 'rdkit2fps --maccs166 '+f_+'.sdf'+' -o '+f_+'.fps'
    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)

os.chdir('/content/results_cvae/fps')

! rm *.fps

os.getcwd()

! wget ftp://ftp.ncbi.nih.gov/pubchem/Compound/CURRENT-Full/SDF/Compound_000000001_000500000.sdf.gz

"""Putting it altogether"""

! rm *.fps

os.chdir('/content/results_cvae/fps')

# Downloading PubChem from ftp in 500K Chunks 
dir = '/fps/'
i=314
while True:
    if i==319:
      break
    try:
      i = i + 1
      url = 'ftp://ftp.ncbi.nih.gov/pubchem/Compound/CURRENT-Full/SDF/Compound_'
      url = url + str(i*500000+1).zfill(9) + '_' + str((i+1)*500000).zfill(9) +'.sdf.gz'
      filename = wget.download(url)
    except:
      continue

! gunzip *.sdf.gz

j=6300

import gzip

# Splitting sdfs
bigsdfs = [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf')].copy()
bigsdfs.sort()

bigsdfs



for i_,f in enumerate(bigsdfs):
    print(i_)
    print(j)
    split_number= 25000
    number_of_sdfs = split_number
    i=0
    name = 'Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf'
    f2=open(name,'w')
    for line in open(f+'.sdf'):
	    f2.write(line)
	    if line[:4] == "$$$$":
		    i+=1
	    if i > number_of_sdfs:
		    number_of_sdfs += split_number 
		    f2.close()
		    j+=1
		    f2=open('Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf','w')
    print(i)
    j=i_*20+6320
    ! rm {f}.sdf

# Removing bigsdfs
for f_r in bigsdfs:
    os.remove(f_r+'.sdf')

! bash /content/conversion.sh

! rdkit2fps --maccs166 Compound_000225001_000250000.sdf -o Compound_000225001_000250000.fps

! cp /content/results_cvae/Compound_005525001_005550000.sdf /content/results_cvae/fps/

os.chdir('/content/results_cvae/')

! rdkit2fps --maccs166 *.sdf -o fps_005500001_009900000.fps

for f_ in [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf')]:
  ! rdkit2fps --maccs166 {f_}.sdf -o {f_}.fps

! cp /content/results_cvae/fps_005500001_009900000.fps /content/drive/MyDrive

# Converting to MACCS 166 bit
import subprocess
sdfs = [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf')]
sdfs.sort()
for f_ in sdfs:
    bashCommand = 'rdkit2fps --maccs166 '+f_+'.sdf'+' -o '+f_+'.fps'
    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)

! rm *.gz

! gzip *.sdf

! gunzip Compound_000025001_000050000.sdf.gz

! rdkit2fps --maccs166 Compound_000025001_000050000.sdf -o Compound_000025001_000050000_1.fps

# Converting to MACCS 166 bit
import subprocess
for f_ in [x[:x.find('.')] for x in os.listdir() if x.endswith('.sdf.gz')]:
    bashCommand = 'rdkit2fps --maccs166 '+f_+'.sdf.gz'+' -o '+f_+'.fps'
    process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)
    print(f_)

os.chdir('/content/results_cvae/fps')

! rm *.sdf

os.chdir('/content/results_cvae/')
! zip -r fps_155000001_157500000.zip fps

! cp /content/results_cvae/fps_155000001_157500000.zip /content/drive/MyDrive

import chemfp
import chemfp.encodings
import chemfp.bitops
import rdkit
from rdkit import Chem
from rdkit import DataStructs
from rdkit.Chem import MACCSkeys
from rdkit.Chem.Fingerprints import FingerprintMols
from rdkit.Chem import Draw
import time
import pandas as pd
import os
from numpy.random import randn

# Required installation : chemfp (python 2.7)
# installed in jupyter virtual environment : source activate chemfp

def search_pubchem(MACCS_bit, thr):
    """
    search
    """
    out=[]
    converted=chemfp.encodings.from_binary_lsb(MACCS_bit)
    for i in range(30):
        a="/fps/Compound_"+str(i*25000+1).zfill(9) + '_' + str((i+1)*25000).zfill(9) +".fps"
        try:
            arena = chemfp.load_fingerprints(a, reorder=False, format="fps")
            out.extend(chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, thr).get_ids_and_scores())
        except:
            i=i
            print ("No such file or directory: " + a)
    return out

def fp_eval_pubchem(filename):
    j=0
    f = open("./"+filename+'.txt', 'r')
    #os.mkdir(filename)
    idx=[]
    num_th_80=[]
    num_th_85 =[]
    num_th_90 =[]
    num_th_95 =[]
    num_th_98 =[]


    while True:
        if j==300:
            break
        j += 1
        line = f.readline()
        if not line: break
        now = line.split(",")
        print(len(now[0]),now[0])
        start_time = time.time()
        res = search_pubchem(now[0], thr=0.8)
        print ("%d: " % (j))
        print ("number of similar compound : %d" % (len(res)))
        print ("--- %s seconds ---" % (time.time() - start_time))
        idx.append(j)

        cid_array = []
        sim_array = []
        for id in res:
            cid_array += [id[0]]
            sim_array += [id[1]]
        cid_array = [int(i) for i in cid_array]
        result = {'CID': cid_array, 'similarity': sim_array}
        a = pd.DataFrame(result)
        a = a.sort_index(by='similarity', ascending=False)
        a.to_csv(filename+"/result_pubchem_" + str(j) + '.csv', index=False)

        num_th_80.append(a['CID'].describe()[0])

        a_over_85 = a.loc[(a['similarity'] > 0.85)]
        num_th_85.append(a_over_85['CID'].describe()[0])

        a_over_90 = a.loc[(a['similarity'] > 0.90)]
        num_th_90.append(a_over_90['CID'].describe()[0])

        a_over_95 = a.loc[(a['similarity'] > 0.95)]
        num_th_95.append(a_over_95['CID'].describe()[0])

        a_over_98 = a.loc[(a['similarity'] > 0.98)]
        num_th_98.append(a_over_98['CID'].describe()[0])

    eval_result = {'count >=0.8': num_th_80, 'count >=0.85': num_th_85, 'count >=0.9': num_th_90, 'count >=0.95': num_th_95, 'count >=0.98': num_th_98}
    df = pd.DataFrame(eval_result, columns=['count >=0.8','count >=0.85','count >=0.9','count >=0.95','count >=0.98'])
    df.to_csv(filename + '/eval.csv', index=False)
    with open(filename + '/log.txt', 'a') as result_log:
        a8=df.loc[(df['count >=0.8'] > 0)]
        a85 = df.loc[(df['count >=0.85'] > 0)]
        a9 = df.loc[(df['count >=0.9'] > 0)]
        a95 = df.loc[(df['count >=0.95'] > 0)]
        a98 = df.loc[(df['count >=0.98'] > 0)]

        result_log.write("number of fingerprint on DB(>=0.8) : %f\n" % (a8['count >=0.8'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.85) : %f\n" % (a85['count >=0.85'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.9) : %f\n" % (a9['count >=0.9'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.95) : %f\n" % (a95['count >=0.95'].describe()[0]))
        result_log.write("number of fingerprint on DB(>=0.98) : %f\n" % (a98['count >=0.98'].describe()[0]))
    f.close()
epoch = "9"
#fp_eval_pubchem("dec_fp_v2_e"+epoch)
#fp_eval_pubchem("dec_fp_v4_e"+epoch)
#fp_eval_pubchem("dec_fp_v6_e"+epoch)
#fp_eval_pubchem("dec_fp_v8_e"+epoch)
#fp_eval_pubchem("dec_fp_v10_e"+epoch)



fp_eval_pubchem("dec_fp_v5_e"+"9")

! wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.8.2-Linux-x86_64.sh
! chmod +x Miniconda3-py37_4.8.2-Linux-x86_64.sh
! bash ./Miniconda3-py37_4.8.2-Linux-x86_64.sh -b -f -p /usr/local

! time conda install -q -y -c openbabel openbabel

! gunzip Compound_000000001_000500000.sdf.gz

! cp /content/results_cvae/Compound_000000001_000500000.sdf /content/results_cvae/fps/Compound_000000001_000500000.sdf



j

os.chdir('/content/results_cvae/fps/')
f= "Compound_000000001_000500000"
split_number= 25000
number_of_sdfs = split_number
i=0
j=0
name = 'Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf'
f2=open(name,'w')
for line in open(f+'.sdf'):
	f2.write(line)
	if line[:4] == "$$$$":
		i+=1
	if i > number_of_sdfs:
		number_of_sdfs += split_number 
		f2.close()
		j+=1
		f2=open('Compound_'+str(j*25000+1).zfill(9)+'_'+str((j+1)*25000).zfill(9)+'.sdf','w')
print(i)

out

out = []
converted=chemfp.encodings.from_binary_lsb('0000000000000000000000000000000000000000000000001000000000000000100001100000001100000000000101111100110010000100100010111101101001101110100001111101011101011110111110')
i=0
a="/fps/Compound_"+str(i*25000+1).zfill(9) + '_' + str((i+1)*25000).zfill(9) +".fps"
arena = chemfp.load_fingerprints('nono.fps', reorder=False, format="fps")
out.extend(chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, 0.7).get_ids_and_scores())

filename = 'dec_fp_v4_e9'
j=0
f = open("./"+filename+'.txt', 'r')
#os.mkdir(filename)
idx=[]
num_th_80=[]
num_th_85 =[]
num_th_90 =[]
num_th_95 =[]
num_th_98 =[]


while True:
    if j==300:
        break
    j += 1
    line = f.readline()
    if not line: break
    now = line.split(",")
    start_time = time.time()
    res = search_pubchem(now[0], thr=0.5)
    print(res)
    print ("%d: " % (j))
    print ("number of similar compound : %d" % (len(res)))
    print ("--- %s seconds ---" % (time.time() - start_time))
    idx.append(j)

    cid_array = []
    sim_array = []
    for id in res:
        cid_array += [id[0]]
        sim_array += [id[1]]
    cid_array = [int(i) for i in cid_array]
    result = {'CID': cid_array, 'similarity': sim_array}
    a = pd.DataFrame(result)
    a = a.sort_index(ascending=False)
    a.to_csv(filename+"/result_pubchem_" + str(j) + '.csv', index=False)

    num_th_80.append(a['CID'].describe()[0])

    a_over_85 = a.loc[(a['similarity'] > 0.85)]
    num_th_85.append(a_over_85['CID'].describe()[0])

    a_over_90 = a.loc[(a['similarity'] > 0.90)]
    num_th_90.append(a_over_90['CID'].describe()[0])

    a_over_95 = a.loc[(a['similarity'] > 0.95)]
    num_th_95.append(a_over_95['CID'].describe()[0])

    a_over_98 = a.loc[(a['similarity'] > 0.98)]
    num_th_98.append(a_over_98['CID'].describe()[0])

eval_result = {'count >=0.8': num_th_80, 'count >=0.85': num_th_85, 'count >=0.9': num_th_90, 'count >=0.95': num_th_95, 'count >=0.98': num_th_98}
df = pd.DataFrame(eval_result, columns=['count >=0.8','count >=0.85','count >=0.9','count >=0.95','count >=0.98'])
df.to_csv(filename + '/eval.csv', index=False)
with open(filename + '/log.txt', 'a') as result_log:
    a8=df.loc[(df['count >=0.8'] > 0)]
    a85 = df.loc[(df['count >=0.85'] > 0)]
    a9 = df.loc[(df['count >=0.9'] > 0)]
    a95 = df.loc[(df['count >=0.95'] > 0)]
    a98 = df.loc[(df['count >=0.98'] > 0)]

    result_log.write("number of fingerprint on DB(>=0.8) : %f\n" % (a8['count >=0.8'].describe()[0]))
    result_log.write("number of fingerprint on DB(>=0.85) : %f\n" % (a85['count >=0.85'].describe()[0]))
    result_log.write("number of fingerprint on DB(>=0.9) : %f\n" % (a9['count >=0.9'].describe()[0]))
    result_log.write("number of fingerprint on DB(>=0.95) : %f\n" % (a95['count >=0.95'].describe()[0]))
    result_log.write("number of fingerprint on DB(>=0.98) : %f\n" % (a98['count >=0.98'].describe()[0]))
f.close()

a

df_ = pd.read_csv('maccs.txt',sep=',')
mfp = df_.loc[0,'FP'].replace(',','')

search_pubchem(df_.loc[0,'FP'].replace(',',''), thr=0.5)

MACCS_bit = mfp

out=[]
converted=chemfp.encodings.from_binary_lsb(MACCS_bit)
for i in range(5323):
    a="./fps/Compound_"+str(i*25000+1).zfill(9) + '_' + str((i+1)*25000).zfill(9) +".fps"
    try:
        arena = chemfp.load_fingerprints(a, reorder=False, format="fps")
        arena.num_bits = 21
        out.extend(chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, thr).get_ids_and_scores())
    except:
        i=i
        print ("No such file or directory: " + a)

arena = chemfp.load_fingerprints(None, reorder=False, format="fps")
arena.num_bits = 167
chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, 0.5).get_ids_and_scores()

arena.num_bits

chemfp.search.threshold_tanimoto_search_fp(converted[1], arena, 0.5)#.get_ids_and_scores()

arena.num_bytes = 21

len(mfp)



"""# ***Similarity with knowns***"""



"""# ***Classifier***"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
# import pandas_profiling
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.feature_selection import chi2,f_classif
from sklearn.linear_model import LogisticRegressionCV
from sklearn.ensemble import RandomForestClassifier as RFC
from sklearn.ensemble import GradientBoostingClassifier as GBC

from sklearn.metrics import plot_roc_curve
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold
import copy
import random
from sklearn.impute import SimpleImputer

import sklearn.metrics

from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score


# %matplotlib inline
# %config InlineBackend.figure_format='retina'

random.seed(121)
class ModelObj:
    def __init__(self,model,x,y,i_train,i_test=None):
        self.model = copy.deepcopy(model)
        self.x_train = x.iloc[i_train]
        self.y_train = y.iloc[i_train]
        if i_test is not None:
            self.x_test = x.iloc[i_test]
            self.y_test = y.iloc[i_test]
        else:
            self.x_test=None
            self.y_test=None
    
    def fit(self):
        self.model.fit(self.x_train,self.y_train)
        self.p_train = self.model.predict_proba(self.x_train)
        if self.x_test is not None:
            self.p_test = self.model.predict_proba(self.x_test)
            self.y_pred = self.model.predict(self.x_test)
        return self

data = pd.DataFrame(train_total_data)
ds_ = data.astype('int64')

random.seed(121)
feature_cols = range(0,167)
rf = RFC(n_estimators=1000,n_jobs=-1,min_samples_leaf=0.001)
kf_ = StratifiedKFold(shuffle=True).split(ds_, ds_[177])
rf = [
    ModelObj(rf, ds_[feature_cols], ds_[177], i, j).fit()
    for i, j in kf_
]

np.sum(rf[0].y_pred)

y_true = rf[2].y_test
y_pred = rf[2].y_pred
y_score = rf[2].model.predict_proba(rf[2].x_test)[:,1]
print(roc_auc_score(y_true,y_score))
confusion_matrix(y_true, y_pred)

from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
X, y = train_total_data[:,:166], train_label
X_test, y_test = test_data[:,:166], test_labels
y_pred = OneVsRestClassifier(LinearSVC(random_state=0)).fit(X, y).predict(X_test)

[y_test[x,:] for x in range(len(y_test)) if y_test[x,:]==y_pred[x,:]]

"""# ***Sandbox***"""

